{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from scipy.special import expit\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in data\n",
    "data = pd.read_csv(\"inputsong3.csv\")\n",
    "\n",
    "# Delete non-number columns\n",
    "data = data.drop(['Artist'], axis=1)\n",
    "data = data.drop(['Album'], axis=1)\n",
    "data = data.drop(['Track ID'], axis=1)\n",
    "data = data.drop(['Track'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Release Date</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>Danceability</th>\n",
       "      <th>Energy</th>\n",
       "      <th>Key Val</th>\n",
       "      <th>Loudness</th>\n",
       "      <th>Mode</th>\n",
       "      <th>Speechiness</th>\n",
       "      <th>Acousticness</th>\n",
       "      <th>Instrumentalness</th>\n",
       "      <th>Liveness</th>\n",
       "      <th>Valence</th>\n",
       "      <th>Tempo</th>\n",
       "      <th>Time Signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013</td>\n",
       "      <td>31</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.972</td>\n",
       "      <td>9</td>\n",
       "      <td>-3.025</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1870</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.102</td>\n",
       "      <td>133.034</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011</td>\n",
       "      <td>73</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.899</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.237</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.004960</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.760</td>\n",
       "      <td>127.086</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1998</td>\n",
       "      <td>39</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.926</td>\n",
       "      <td>7</td>\n",
       "      <td>-5.954</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1270</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.466000</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.777</td>\n",
       "      <td>168.284</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1998</td>\n",
       "      <td>30</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.868</td>\n",
       "      <td>11</td>\n",
       "      <td>-6.360</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0613</td>\n",
       "      <td>0.002470</td>\n",
       "      <td>0.174000</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.485</td>\n",
       "      <td>110.950</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000</td>\n",
       "      <td>57</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.664</td>\n",
       "      <td>5</td>\n",
       "      <td>-7.715</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0542</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.630</td>\n",
       "      <td>87.287</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Release Date  Popularity  Danceability  Energy  Key Val  Loudness  Mode  \\\n",
       "0          2013          31         0.455   0.972        9    -3.025     1   \n",
       "1          2011          73         0.733   0.899        0    -4.237     1   \n",
       "2          1998          39         0.632   0.926        7    -5.954     1   \n",
       "3          1998          30         0.747   0.868       11    -6.360     0   \n",
       "4          2000          57         0.690   0.664        5    -7.715     0   \n",
       "\n",
       "   Speechiness  Acousticness  Instrumentalness  Liveness  Valence    Tempo  \\\n",
       "0       0.1870      0.000193          0.000229     0.387    0.102  133.034   \n",
       "1       0.1430      0.004960          0.000047     0.372    0.760  127.086   \n",
       "2       0.1270      0.008900          0.466000     0.367    0.777  168.284   \n",
       "3       0.0613      0.002470          0.174000     0.494    0.485  110.950   \n",
       "4       0.0542      0.000447          0.034300     0.073    0.630   87.287   \n",
       "\n",
       "   Time Signature  \n",
       "0               4  \n",
       "1               4  \n",
       "2               4  \n",
       "3               4  \n",
       "4               4  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_di = {0: \"C\",0: \"B#\", 1: \"C#\", 2: \"D\", 3: \"D#\", 4: \"E\", 5: \"F\", 5: \"E#\", 6: \"F#\", \n",
    "          7: \"G\", 8: \"G#\", 9: \"A\", 10: \"A#\", 11: \"B\"}\n",
    "data.replace({\"Key\": key_di})\n",
    "data = data.drop(['Key'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, validate_data, test_data = np.split(data.sample(frac=1), [int(.8*len(data)), int(.9*len(data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48835\n",
      "6104\n",
      "6105\n"
     ]
    }
   ],
   "source": [
    "train_output = train_data.Popularity\n",
    "train_data = train_data.drop(['Popularity'], axis=1)\n",
    "train_data = np.asarray(train_data,dtype=np.float64)\n",
    "training_count = len(train_data[:,0])\n",
    "\n",
    "validate_output = validate_data.Popularity\n",
    "validate_data = validate_data.drop(['Popularity'], axis=1)\n",
    "validate_data = np.asarray(validate_data,dtype=np.float64)\n",
    "validate_count = len(validate_data[:,0])\n",
    "\n",
    "test_output = test_data.Popularity\n",
    "test_data = test_data.drop(['Popularity'], axis=1)\n",
    "test_data = np.asarray(test_data,dtype=np.float64)\n",
    "test_count = len(test_data[:,0])\n",
    "print(training_count)\n",
    "print(validate_count)\n",
    "print(test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lin_peihsuan/opt/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
       "              beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=1000,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=None, shuffle=True, solver='lbfgs',\n",
       "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#class sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(100, ), \n",
    "# activation='relu', solver='adam', alpha=0.0001, batch_size='auto', \n",
    "# learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, \n",
    "# shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, \n",
    "# momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, \n",
    "# beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000)\n",
    "\n",
    "mlp = MLPClassifier(solver='lbfgs', activation='logistic',max_iter=1000)\n",
    "mlp.fit(train_data,train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0  0  0  0  0  0  0  0  0  0 21  0  0  0  0  0 21  0]\n"
     ]
    }
   ],
   "source": [
    "y_pred = mlp.predict(train_data)\n",
    "print(y_pred[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.03589638578888093\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set score: {}\".format(mlp.score(train_data,train_output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing set score: 0.03734643734643735\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing set score: {}\".format(mlp.score(test_data,test_output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set score: 0.035222804718217565\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation set score: {}\".format(mlp.score(validate_data,validate_output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing set score: [[2.96551043e-02 1.63444616e-02 1.61396550e-02 1.59173986e-02\n",
      "  1.67600551e-02 1.67878422e-02 1.78428417e-02 1.76539546e-02\n",
      "  1.80338210e-02 1.79643915e-02 1.91179276e-02 1.80881888e-02\n",
      "  1.93285375e-02 1.78201640e-02 1.91516800e-02 1.82276750e-02\n",
      "  1.67746594e-02 1.92417124e-02 2.02064424e-02 2.21774027e-02\n",
      "  2.15315286e-02 2.34004832e-02 2.09682851e-02 2.22062707e-02\n",
      "  2.07264933e-02 2.08174119e-02 1.96909035e-02 2.02398724e-02\n",
      "  1.99298268e-02 1.85991507e-02 1.82372775e-02 1.67503326e-02\n",
      "  1.73966230e-02 1.70047550e-02 1.78661894e-02 1.65504043e-02\n",
      "  1.75022478e-02 1.57382706e-02 1.42059133e-02 1.45834693e-02\n",
      "  1.23555733e-02 1.32664631e-02 1.33224682e-02 1.25340778e-02\n",
      "  1.11240002e-02 1.15579268e-02 1.21271747e-02 1.07541426e-02\n",
      "  1.11535624e-02 9.33540600e-03 1.06747924e-02 1.03308445e-02\n",
      "  7.71293921e-03 7.94686654e-03 6.83872211e-03 6.92252269e-03\n",
      "  6.51344789e-03 6.63628410e-03 5.33125843e-03 6.48855694e-03\n",
      "  5.93106580e-03 4.68564206e-03 4.59046787e-03 4.34647075e-03\n",
      "  4.07629235e-03 4.28925026e-03 3.83030644e-03 3.48434560e-03\n",
      "  3.72367584e-03 2.61907444e-03 3.41075794e-03 2.43454084e-03\n",
      "  2.32389607e-03 2.53001506e-03 1.48430443e-03 1.07915918e-03\n",
      "  1.04607576e-03 8.84521306e-04 5.11864397e-04 4.47568530e-04\n",
      "  2.94587631e-04 3.05191289e-04 3.81574499e-04 6.83013358e-05\n",
      "  9.74407883e-05 1.50308540e-04 5.07128593e-05 5.93775966e-04\n",
      "  1.42359999e-04 2.88210752e-05 3.18405812e-05 5.76622563e-06\n",
      "  7.15675350e-06 6.21331359e-06]\n",
      " [2.86763116e-02 1.58160426e-02 1.57005748e-02 1.53942042e-02\n",
      "  1.64301133e-02 1.64305847e-02 1.73445559e-02 1.72186952e-02\n",
      "  1.75509092e-02 1.75725335e-02 1.88852612e-02 1.78145505e-02\n",
      "  1.89961095e-02 1.73816810e-02 1.88399675e-02 1.79528385e-02\n",
      "  1.65449006e-02 1.88404132e-02 1.98523737e-02 2.19486832e-02\n",
      "  2.12963716e-02 2.31869441e-02 2.07315334e-02 2.21009440e-02\n",
      "  2.07622291e-02 2.07893127e-02 1.96118088e-02 2.00491234e-02\n",
      "  2.00012952e-02 1.85349653e-02 1.82821579e-02 1.70398275e-02\n",
      "  1.75098746e-02 1.71977062e-02 1.80740077e-02 1.68247615e-02\n",
      "  1.77839677e-02 1.60915523e-02 1.45283188e-02 1.47007703e-02\n",
      "  1.26165475e-02 1.34791383e-02 1.36248513e-02 1.28574586e-02\n",
      "  1.14422914e-02 1.19184011e-02 1.23771050e-02 1.10809042e-02\n",
      "  1.16062134e-02 9.64400679e-03 1.09825565e-02 1.05930629e-02\n",
      "  7.92697064e-03 8.16994201e-03 7.05050477e-03 7.10428089e-03\n",
      "  6.73873442e-03 6.86050832e-03 5.49332376e-03 6.69858305e-03\n",
      "  6.13459315e-03 4.81795790e-03 4.69760201e-03 4.47697091e-03\n",
      "  4.17604542e-03 4.40198173e-03 3.94728589e-03 3.57780331e-03\n",
      "  3.81523142e-03 2.69585275e-03 3.50285954e-03 2.49740025e-03\n",
      "  2.37284109e-03 2.59048234e-03 1.50817243e-03 1.10223637e-03\n",
      "  1.06692564e-03 9.04022104e-04 5.19704017e-04 4.53961688e-04\n",
      "  2.96938115e-04 3.09093514e-04 3.87963912e-04 6.89533739e-05\n",
      "  9.80515626e-05 1.51513186e-04 5.11024291e-05 5.98880216e-04\n",
      "  1.43068757e-04 2.89515722e-05 3.20977400e-05 5.79786142e-06\n",
      "  7.22296415e-06 6.27391995e-06]]\n"
     ]
    }
   ],
   "source": [
    "#probability to each label\n",
    "print(\"Testing set score: {}\".format(mlp.predict_proba(validate_data[:2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.04      0.99      0.07       205\n",
      "           1       0.00      0.00      0.00       116\n",
      "           2       0.00      0.00      0.00       118\n",
      "           3       0.00      0.00      0.00       128\n",
      "           4       0.00      0.00      0.00       113\n",
      "           5       0.00      0.00      0.00       111\n",
      "           6       0.00      0.00      0.00       103\n",
      "           7       0.00      0.00      0.00       111\n",
      "           8       0.00      0.00      0.00       113\n",
      "           9       0.00      0.00      0.00       105\n",
      "          10       0.00      0.00      0.00       120\n",
      "          11       0.00      0.00      0.00       128\n",
      "          12       0.00      0.00      0.00       132\n",
      "          13       0.00      0.00      0.00       132\n",
      "          14       0.00      0.00      0.00       124\n",
      "          15       0.00      0.00      0.00       129\n",
      "          16       0.00      0.00      0.00       106\n",
      "          17       0.00      0.00      0.00       129\n",
      "          18       0.00      0.00      0.00       136\n",
      "          19       0.00      0.00      0.00       150\n",
      "          20       0.00      0.00      0.00       130\n",
      "          21       0.02      0.03      0.02       134\n",
      "          22       0.00      0.00      0.00       122\n",
      "          23       0.00      0.00      0.00       130\n",
      "          24       0.02      0.02      0.02       125\n",
      "          25       0.00      0.00      0.00       139\n",
      "          26       0.00      0.00      0.00       143\n",
      "          27       0.00      0.00      0.00       109\n",
      "          28       0.00      0.00      0.00       124\n",
      "          29       0.00      0.00      0.00       120\n",
      "          30       0.00      0.00      0.00       116\n",
      "          31       0.00      0.00      0.00        91\n",
      "          32       0.00      0.00      0.00       113\n",
      "          33       0.00      0.00      0.00        84\n",
      "          34       0.00      0.00      0.00        93\n",
      "          35       0.00      0.00      0.00        87\n",
      "          36       0.03      0.07      0.05        81\n",
      "          37       0.00      0.00      0.00        79\n",
      "          38       0.00      0.00      0.00        80\n",
      "          39       0.00      0.00      0.00        59\n",
      "          40       0.00      0.00      0.00        78\n",
      "          41       0.00      0.00      0.00        67\n",
      "          42       0.00      0.00      0.00        85\n",
      "          43       0.00      0.00      0.00        84\n",
      "          44       0.00      0.00      0.00        82\n",
      "          45       0.00      0.00      0.00        74\n",
      "          46       0.00      0.00      0.00        74\n",
      "          47       0.00      0.00      0.00        61\n",
      "          48       0.00      0.00      0.00        62\n",
      "          49       0.00      0.00      0.00        50\n",
      "          50       0.00      0.00      0.00        54\n",
      "          51       0.00      0.00      0.00        45\n",
      "          52       0.00      0.00      0.00        36\n",
      "          53       0.00      0.00      0.00        44\n",
      "          54       0.00      0.00      0.00        27\n",
      "          55       0.00      0.00      0.00        29\n",
      "          56       0.00      0.00      0.00        41\n",
      "          57       0.00      0.00      0.00        31\n",
      "          58       0.00      0.00      0.00        27\n",
      "          59       0.00      0.00      0.00        22\n",
      "          60       0.00      0.00      0.00        37\n",
      "          61       0.00      0.00      0.00        27\n",
      "          62       0.00      0.00      0.00        29\n",
      "          63       0.00      0.00      0.00        27\n",
      "          64       0.00      0.00      0.00        30\n",
      "          65       0.00      0.00      0.00        24\n",
      "          66       0.00      0.00      0.00        22\n",
      "          67       0.00      0.00      0.00        22\n",
      "          68       0.00      0.00      0.00        24\n",
      "          69       0.00      0.00      0.00        14\n",
      "          70       0.00      0.00      0.00        21\n",
      "          71       0.00      0.00      0.00         8\n",
      "          72       0.00      0.00      0.00        19\n",
      "          73       0.00      0.00      0.00         9\n",
      "          74       0.00      0.00      0.00         3\n",
      "          75       0.00      0.00      0.00        12\n",
      "          76       0.00      0.00      0.00        10\n",
      "          77       0.00      0.00      0.00         7\n",
      "          78       0.00      0.00      0.00         4\n",
      "          79       0.00      0.00      0.00         2\n",
      "          80       0.00      0.00      0.00         3\n",
      "          81       0.00      0.00      0.00         3\n",
      "          82       0.00      0.00      0.00         1\n",
      "          84       0.00      0.00      0.00         2\n",
      "          85       0.00      0.00      0.00         2\n",
      "          86       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.04      6104\n",
      "   macro avg       0.00      0.01      0.00      6104\n",
      "weighted avg       0.00      0.04      0.00      6104\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lin_peihsuan/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(validate_output,mlp.predict(validate_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score of training: 0.03589638578888093\n",
      "accuracy score of testing: 0.03734643734643735\n",
      "accuracy score of validating: 0.035222804718217565\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy score of training: {}\".format(accuracy_score(train_output,mlp.predict(train_data))))\n",
    "print(\"accuracy score of testing: {}\".format(accuracy_score(test_output,mlp.predict(test_data))))\n",
    "print(\"accuracy score of validating: {}\".format(accuracy_score(validate_output,mlp.predict(validate_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
